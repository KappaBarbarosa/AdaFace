{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfxjYKvsazZq",
        "outputId": "0b161f69-5168-49e7-b985-104437e6eb3a"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGrZOje_DspU",
        "outputId": "33957cd7-3c27-47b6-bfea-503c687d0848"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/AdaFace/\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWCMjTgYyhcM",
        "outputId": "7f00264a-094a-4675-a6f9-5c2454cb226f"
      },
      "outputs": [],
      "source": [
        "# %pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFkbhbpCbfnS",
        "outputId": "ea058752-190a-42f8-dc2c-97742043971f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)], sys.version_info(major=3, minor=11, micro=0, releaselevel='final', serial=0) \n",
            "Pytorch version: 2.0.0+cu118 \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
        "print(f\"Pytorch version: {torch.__version__} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBwdlvWbbsUw",
        "outputId": "e60ffad8-422d-4f5d-fb8e-6633a8307560"
      },
      "outputs": [],
      "source": [
        "# !python build_database.py --database_savedir './AdaFace/database_result'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y5agJRWpFzlA"
      },
      "outputs": [],
      "source": [
        "sys.path.append('./AdaFace/')\n",
        "import net\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "# from face_alignment import align\n",
        "from yolo_face.align import YOLO_FACE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HbrSB7QkMC_S"
      },
      "outputs": [],
      "source": [
        "adaface_models = {\n",
        "    'ir_50':\"pretrained/adaface_ir50_ms1mv2.ckpt\",\n",
        "    'ir_101':\"pretrained/adaface_ir101_ms1mv2.ckpt\",\n",
        "}\n",
        "def load_pretrained_model(architecture='ir_50',path=''):\n",
        "    # load model and pretrained statedict\n",
        "    assert architecture in adaface_models.keys()\n",
        "    model = net.build_model(architecture)\n",
        "    pt = path if path is  not None else adaface_models[architecture]\n",
        "    statedict = torch.load(pt)['state_dict']\n",
        "    model_statedict = {key[6:]:val for key, val in statedict.items() if key.startswith('model.')}\n",
        "    model.load_state_dict(model_statedict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def to_input(pil_rgb_image, RGB=True):\n",
        "    np_img = np.array(pil_rgb_image)\n",
        "    print(\"np_img shape:\", np_img.shape)  # 新增這行\n",
        "    if RGB:\n",
        "        brg_img = ((np_img / 255.) - 0.5) / 0.5\n",
        "    else:\n",
        "        brg_img = ((np_img[:, :, ::-1] / 255.) - 0.5) / 0.5\n",
        "    print(\"brg_img shape:\", brg_img.shape)  # 新增這行\n",
        "    tensor = torch.tensor(np_img.transpose(0, 3, 1, 2)).float()\n",
        "    return tensor.cpu().numpy()  # 將張量轉換為 NumPy 陣列\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\User\\\\Desktop\\\\study\\\\_ML\\\\AdaFace'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmy8vSu6Nruj",
        "outputId": "7b24aff5-c892-4b66-98ec-e0f934406abc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
            "  warnings.warn(\"No audio backend is available.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "face recongnition model loaded\n",
            "face detection model loaded\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x268ad12e390>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fr_weights = 'c:/Users/User/Desktop/study/_ML/AdaFace/pretrained/adaface_ir50_ms1mv2.ckpt'\n",
        "fd_weights = 'c:/Users/User/Desktop/study/_ML/AdaFace/pretrained/yolov7-tiny.pt'\n",
        "isyolo = True\n",
        "arch = 'ir_50'\n",
        "database_path = 'face_database'\n",
        "database_dir =  './AdaFace/database_result' #validation sets\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = load_pretrained_model(arch,path=fr_weights).to(device)\n",
        "print('face recongnition model loaded')\n",
        "yoloface = YOLO_FACE(fd_weights,device=device) if isyolo else None\n",
        "print('face detection model loaded')\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOTTv_saRdK9"
      },
      "source": [
        "# Firebase Firestore Database Initialization & Function Definition\n",
        "- create_class(): no need to call by user\n",
        "- student_arrive(): detect a student's face at the moment, parameter is student's ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install --user firebase-admin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DJP0OaYxt-Cx"
      },
      "outputs": [],
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import asyncio\n",
        "\n",
        "cred = credentials.Certificate(\"./serviceAccountKey.json\")\n",
        "firebase_admin.initialize_app(cred)\n",
        "\n",
        "\n",
        "# 初始化firestore\n",
        "db = firestore.client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8q3Ze8LPQsaS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_class(class_date):\n",
        "    student_id_list_ref = db.collection(\"students\").document(\"student_id_list\")\n",
        "    class_doc_ref = db.collection(\"classes\").document(str(class_date))\n",
        "    class_doc_ref.set(student_id_list_ref.get().to_dict())\n",
        "\n",
        "def student_arrive(student_id):\n",
        "    cur_date = datetime.now(pytz.timezone('Asia/Taipei')).strftime(\"%m%d\")\n",
        "    cur_time = datetime.now(pytz.timezone('Asia/Taipei')).strftime(\"%H:%M:%S\")\n",
        "    class_ref = db.collection(\"classes\").document(str(cur_date))\n",
        "    snapshot = class_ref.get()\n",
        "    snapshot_dict = snapshot.to_dict()\n",
        "    class_update_data = {\n",
        "        student_id: cur_time\n",
        "    }\n",
        "    if class_ref.get().exists == False:\n",
        "        create_class(cur_date)\n",
        "    elif type(snapshot_dict[student_id]) == str:\n",
        "        print(f\"{student_id} had arrived at {snapshot_dict[student_id]}\")\n",
        "        return\n",
        "    class_ref.update(class_update_data)\n",
        "    ##########\n",
        "    student_document_ref = db.collection(\"students\").document(\"student_id_list\").collection(\"ML\").document(student_id)\n",
        "    student_update_data = {\n",
        "        cur_date : cur_time\n",
        "    }\n",
        "    if student_document_ref.get().exists == False:\n",
        "        student_document_ref.set(student_update_data)\n",
        "    else:\n",
        "        student_document_ref.update(student_update_data)\n",
        "\n",
        "    print(f\"update successed. {student_id} arrives at {cur_date} {cur_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpwWhxCaO0vb"
      },
      "source": [
        "# INPUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DLC1hljVsO_"
      },
      "source": [
        "## WebCam setting\n",
        "[REFERENCE](https://colab.research.google.com/github/OmniXRI/Colab_Webcam_OpenCV/blob/main/Colab_Webcam_OpenCV.ipynb#scrollTo=nLxYK8ccMMI-)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q9Q_e6JCX1n-"
      },
      "outputs": [],
      "source": [
        "# https://colab.research.google.com/github/OmniXRI/Colab_Webcam_OpenCV/blob/main/Colab_Webcam_OpenCV.ipynb#scrollTo=nLxYK8ccMMI-\n",
        "from IPython.display import display, Javascript, Image\n",
        "# from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import subprocess\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MToawQgGYqIQ"
      },
      "outputs": [],
      "source": [
        "def js_to_image(js_reply):\n",
        "  # 解碼成 base64 格式影像\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # 轉換影像變成 Numpy 格式\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # 解碼 Numpy格式到 OpenCV BGR 影像格式\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "def image_to_js(cv2_img):\n",
        "  _, buffer = cv2.imencode('.jpg', cv2_img)\n",
        "  image_str = b64encode(buffer).decode('utf-8')\n",
        "\n",
        "  # show img\n",
        "  display(Javascript(\"\"\"\n",
        "      var img = document.createElement('img');\n",
        "      img.src = 'data:image/jpeg;base64,{}';\n",
        "      document.body.appendChild(img);\n",
        "  \"\"\".format(image_str)))\n",
        "\n",
        "def img_process(img):\n",
        "\n",
        "    bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    return bgr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def video_frame(label, bbox):\n",
        "  # data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  result = subprocess.run(['node', 'c:/Users/User/Desktop/study/_ML/AdaFace/video_stream.js', label, bbox], capture_output=True, text=True)\n",
        "  print(\"stdout:\", result.stdout)\n",
        "  print(\"stderr:\", result.stderr)\n",
        "  result.check_returncode()\n",
        "  data = result.stdout.strip()\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szWLXbiOYqwj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaeoLbQ1Vgpu"
      },
      "source": [
        "## Create Video Stream and Start Detection\n",
        "\n",
        "now detect every 5 seconds\n",
        "want to improve to approximately real-time\n",
        "and improve the representation in the scheme\n",
        "\n",
        "-\n",
        "\n",
        "想要可以把人框起來上面寫人名跟score的那種酷效果\n",
        "可能要改yolo_face回傳的參數\n",
        "\n",
        "yolo_face裡面有個detect video還是什麼的但那在colab上用不了Q\n",
        "\n",
        "-\n",
        "\n",
        "1201 update:\n",
        "\n",
        "1. 可以一次測兩個人(歡呼)但我沒有第二台手機沒辦法測第三個人，兩個人ok的話三個人應該也行吧\n",
        "\n",
        "2. firestore database只會紀錄第一次arrive的時間\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "WBlEB-2dY_4_",
        "outputId": "ae9f3482-fb52-4ea2-edc1-49365752e966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected faces number:  2\n",
            "np_img shape: (2, 112, 112, 3)\n",
            "brg_img shape: (2, 112, 112, 3)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot select an axis to squeeze out which has size not equal to one",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected faces number: \u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;28mlen\u001b[39m(aligned_bgr_imgs))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aligned_bgr_imgs) :\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# imgs.append(tt[i])\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# cv2.imwrite(f'deteced_{ct}_{i}.jpg',img)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     bgr_tensor_input \u001b[38;5;241m=\u001b[39m \u001b[43mto_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     41\u001b[0m     feature, _ \u001b[38;5;241m=\u001b[39m model(bgr_tensor_input)\n\u001b[0;32m     42\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(feature)\n",
            "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mto_input\u001b[1;34m(pil_rgb_image, RGB)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrg_img shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, brg_img\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# 新增這行\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 將多餘的維度去掉\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m brg_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mexpand_dims(brg_img\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1558\u001b[0m, in \u001b[0;36msqueeze\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m   1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m squeeze()\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
          ]
        }
      ],
      "source": [
        "# 啟動網路攝影機開始接收串流影像\n",
        "# video_stream()\n",
        "cap = cv2.VideoCapture(cv2.CAP_DSHOW)\n",
        "# 設定串流影像文字標籤\n",
        "label_html = 'Capturing...'\n",
        "# 清空疊合影像內容\n",
        "overlap_img = ''\n",
        "source = './test_images'\n",
        "features = []\n",
        "face=[]\n",
        "imgs=[]\n",
        "test_image = sorted(os.listdir(source))\n",
        "ct=0\n",
        "# 執行取像、處理循環，執行過程中若於影像上按下滑鼠鍵即會中止程式\n",
        "cam_start_time = time.time()\n",
        "while True:\n",
        "    # 從網路攝影機取得串流影像目前影格\n",
        "    # js_reply = video_frame(label_html, overlap_img)\n",
        "    ret, frame = cap.read()\n",
        "    # 若無法取得影格則結束循環\n",
        "    if not ret:\n",
        "        print(\"nothing\")\n",
        "        break\n",
        "\n",
        "    # 將回傳的JavaScript影像物件轉成 OpenCV BGR 格式\n",
        "    # js_img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    cur_time = time.time()\n",
        "\n",
        "    if cur_time - cam_start_time >= 5:\n",
        "        cam_start_time = cur_time\n",
        "\n",
        "        if isyolo:\n",
        "            # aligned_bgr_imgs = yoloface.frame_detect(js_img)\n",
        "            aligned_bgr_imgs = yoloface.frame_detect(frame)\n",
        "            print(\"Detected faces number: \" , len(aligned_bgr_imgs))\n",
        "            for i,img in enumerate(aligned_bgr_imgs) :\n",
        "                # imgs.append(tt[i])\n",
        "                # cv2.imwrite(f'deteced_{ct}_{i}.jpg',img)\n",
        "                bgr_tensor_input = to_input(img,True).to(device)\n",
        "                feature, _ = model(bgr_tensor_input)\n",
        "                features.append(feature)\n",
        "                face.append(f'deteced_{ct}_{i}.jpg')\n",
        "            ct+=1\n",
        "        else:\n",
        "            # aligned_rgb_img = align.get_aligned_face(js_img, isarray=True)\n",
        "            aligned_rgb_img = align.get_aligned_face(frame, isarray=True)\n",
        "            if(aligned_rgb_img is not None):\n",
        "                bgr_tensor_input = to_input(aligned_rgb_img,False).to(device)\n",
        "                feature, _ = model(bgr_tensor_input)\n",
        "                features.append(feature)\n",
        "\n",
        "        # if nothing detected\n",
        "        if features == []:\n",
        "            continue\n",
        "\n",
        "        database_dir =  'c:/Users/User/Desktop/study/_ML/AdaFace/database_result'\n",
        "        database_pt_dir = os.path.join(database_dir,'database.pt')\n",
        "        if(os.path.exists(database_pt_dir)):\n",
        "            database = torch.load(database_pt_dir).to(device)\n",
        "            with open(os.path.join(database_dir, 'ID_list.txt'), 'r') as file:\n",
        "                ID_list = [line.strip() for line in file.readlines()]\n",
        "            start_time = time.time()\n",
        "            with torch.no_grad():\n",
        "                similarity_scores = torch.cat(features) @ database.T\n",
        "            end_time = time.time()\n",
        "            execution_time = end_time - start_time\n",
        "            print(f\"computing time: {execution_time} seconds\")\n",
        "            score = similarity_scores.cpu().detach().numpy()\n",
        "            max_index = np.argmax(score, axis=1)\n",
        "            # print(\"max_index: \",  max_index, \"\\nlen(imgs): \", len(imgs))\n",
        "\n",
        "            for i,id in enumerate(max_index):\n",
        "                print(f'in {face[i]}')\n",
        "                if(score[i][id] >= 0.3):\n",
        "                    img_with_text = aligned_bgr_imgs[i]\n",
        "                    face_text = f'{ID_list[int(id/3)]}-Score:{score[i][id]:.2f}'\n",
        "                    cv2.putText(img_with_text, face_text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)\n",
        "                    # plt.figure(figsize=(12,4))\n",
        "                    # img_with_text_rgb = cv2.cvtColor(img_with_text, cv2.COLOR_BGR2RGB)\n",
        "                    display(Image(data=cv2.imencode('.png', img_with_text)[1].tobytes(), format='png'))\n",
        "                    student_arrive(str(ID_list[int(id/3)]))\n",
        "\n",
        "                else:\n",
        "                    print(\"no ensured answer\")\n",
        "                    img_with_text = aligned_bgr_imgs[i]\n",
        "                    face_text = \"no ensured answer\"\n",
        "                    cv2.putText(img_with_text, face_text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)\n",
        "                    display(Image(data=cv2.imencode('.png', img_with_text)[1].tobytes(), format='png'))\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                similarity_scores = torch.cat(features) @ torch.cat(features).T\n",
        "            print(similarity_scores)\n",
        "\n",
        "        features = []\n",
        "        face=[]\n",
        "        imgs=[]\n",
        "    cv2.imshow('Video Stream', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJjAdkeIYrgj"
      },
      "source": [
        "# ORIGINAL CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlMiw_DooZl9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzubGzZEMOEw"
      },
      "outputs": [],
      "source": [
        "# source = 'test_images'\n",
        "# features = []\n",
        "# face=[]\n",
        "# imgs=[]\n",
        "# test_image = sorted(os.listdir(source))\n",
        "# ct=0\n",
        "# for fname in test_image:\n",
        "#     path = os.path.join(source, fname)\n",
        "#     if isyolo:\n",
        "#         aligned_bgr_imgs,tt = yoloface.show_detect(path)\n",
        "#         for i,img in enumerate(aligned_bgr_imgs) :\n",
        "#             imgs.append(tt[i])\n",
        "#             # cv2.imwrite(f'deteced_{ct}_{i}.jpg',img)\n",
        "#             bgr_tensor_input = to_input(img,True).to(device)\n",
        "#             feature, _ = model(bgr_tensor_input)\n",
        "#             features.append(feature)\n",
        "#             face.append(f'deteced_{ct}_{i}.jpg')\n",
        "#         ct+=1\n",
        "#     else:\n",
        "#         aligned_rgb_img = align.get_aligned_face(path, isarray=True)\n",
        "#         if(aligned_rgb_img is not None):\n",
        "#             bgr_tensor_input = to_input(aligned_rgb_img,False).to(device)\n",
        "#             feature, _ = model(bgr_tensor_input)\n",
        "#             features.append(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c64kBjOQLfJ"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import display, Image\n",
        "\n",
        "# database_dir =  '/content/drive/MyDrive/AdaFace/database_result'\n",
        "# database_pt_dir = os.path.join(database_dir,'database.pt')\n",
        "# if(os.path.exists(database_pt_dir)):\n",
        "#     database = torch.load(database_pt_dir).to(device)\n",
        "#     with open(os.path.join(database_dir, 'ID_list.txt'), 'r') as file:\n",
        "#         ID_list = [line.strip() for line in file.readlines()]\n",
        "#     start_time = time.time()\n",
        "#     with torch.no_grad():\n",
        "#         similarity_scores = torch.cat(features) @ database.T\n",
        "#     end_time = time.time()\n",
        "#     execution_time = end_time - start_time\n",
        "#     print(f\"computing time: {execution_time} seconds\")\n",
        "#     score = similarity_scores.cpu().detach().numpy()\n",
        "#     max_index = np.argmax(score, axis=1)\n",
        "\n",
        "#     for i,id in enumerate(max_index):\n",
        "#         print(f'in {face[i]}')\n",
        "#         if(score[i][id] >= 0.3):\n",
        "#             img_with_text = imgs[i].copy()\n",
        "#             face_text = f'{ID_list[int(id/3)]}-Score:{score[i][id]:.2f}'\n",
        "#             cv2.putText(img_with_text, face_text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "#             # plt.figure(figsize=(12,4))\n",
        "#             # img_with_text_rgb = cv2.cvtColor(img_with_text, cv2.COLOR_BGR2RGB)\n",
        "#             display(Image(data=cv2.imencode('.png', img_with_text)[1].tobytes(), format='png'))\n",
        "#             student_arrive(str(ID_list[int(id/3)]))\n",
        "#         else:\n",
        "#             print(\"no ensured answer\")\n",
        "# else:\n",
        "#     with torch.no_grad():\n",
        "#         similarity_scores = torch.cat(features) @ torch.cat(features).T\n",
        "#     print(similarity_scores)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
