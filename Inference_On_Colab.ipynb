{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfxjYKvsazZq",
        "outputId": "0b161f69-5168-49e7-b985-104437e6eb3a"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGrZOje_DspU",
        "outputId": "33957cd7-3c27-47b6-bfea-503c687d0848"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/AdaFace/\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWCMjTgYyhcM",
        "outputId": "7f00264a-094a-4675-a6f9-5c2454cb226f"
      },
      "outputs": [],
      "source": [
        "# %pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFkbhbpCbfnS",
        "outputId": "ea058752-190a-42f8-dc2c-97742043971f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)], sys.version_info(major=3, minor=11, micro=0, releaselevel='final', serial=0) \n",
            "Pytorch version: 2.0.0+cu118 \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
        "print(f\"Pytorch version: {torch.__version__} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBwdlvWbbsUw",
        "outputId": "e60ffad8-422d-4f5d-fb8e-6633a8307560"
      },
      "outputs": [],
      "source": [
        "# !python build_database.py --database_savedir './AdaFace/face_database'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y5agJRWpFzlA"
      },
      "outputs": [],
      "source": [
        "sys.path.append('./AdaFace/')\n",
        "import net\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "from yolo_face.align import YOLO_FACE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HbrSB7QkMC_S"
      },
      "outputs": [],
      "source": [
        "adaface_models = {\n",
        "    'ir_50':\"pretrained/adaface_ir50_ms1mv2.ckpt\",\n",
        "    'ir_101':\"pretrained/adaface_ir101_ms1mv2.ckpt\",\n",
        "}\n",
        "def load_pretrained_model(architecture='ir_50',path=''):\n",
        "    # load model and pretrained statedict\n",
        "    assert architecture in adaface_models.keys()\n",
        "    model = net.build_model(architecture)\n",
        "    pt = path if path is  not None else adaface_models[architecture]\n",
        "    statedict = torch.load(pt)['state_dict']\n",
        "    model_statedict = {key[6:]:val for key, val in statedict.items() if key.startswith('model.')}\n",
        "    model.load_state_dict(model_statedict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def to_input(pil_rgb_image,RGB=True):\n",
        "    np_img = np.array(pil_rgb_image)\n",
        "    if RGB:\n",
        "        brg_img = ((np_img / 255.) - 0.5) / 0.5\n",
        "    else :\n",
        "        brg_img = ((np_img[:,:,::-1] / 255.) - 0.5) / 0.5\n",
        "    tensor = torch.tensor(np.expand_dims(brg_img.transpose(2,0,1),axis=0)).float()\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\User\\\\Desktop\\\\study\\\\_ML\\\\AdaFace'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmy8vSu6Nruj",
        "outputId": "7b24aff5-c892-4b66-98ec-e0f934406abc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
            "  warnings.warn(\"No audio backend is available.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "face recongnition model loaded\n",
            "face detection model loaded\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x165291651d0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fr_weights = 'c:/Users/User/Desktop/study/_ML/AdaFace/pretrained/adaface_ir50_ms1mv2.ckpt'\n",
        "fd_weights = 'c:/Users/User/Desktop/study/_ML/AdaFace/pretrained/yolov7-tiny.pt'\n",
        "isyolo = True\n",
        "arch = 'ir_50'\n",
        "database_path = 'face_database'\n",
        "database_dir =  './AdaFace/face_database' #validation sets\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = load_pretrained_model(arch,path=fr_weights).to(device)\n",
        "print('face recongnition model loaded')\n",
        "yoloface = YOLO_FACE(fd_weights,device=device) if isyolo else None\n",
        "print('face detection model loaded')\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOTTv_saRdK9"
      },
      "source": [
        "# Firebase Firestore Database Initialization & Function Definition\n",
        "- create_class(): no need to call by user\n",
        "- student_arrive(): detect a student's face at the moment, parameter is student's ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install --user firebase-admin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DJP0OaYxt-Cx"
      },
      "outputs": [],
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import asyncio\n",
        "\n",
        "cred = credentials.Certificate(\"./serviceAccountKey.json\")\n",
        "firebase_admin.initialize_app(cred)\n",
        "\n",
        "\n",
        "# 初始化firestore\n",
        "db = firestore.client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8q3Ze8LPQsaS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_class(class_date):\n",
        "    student_id_list_ref = db.collection(\"students\").document(\"student_id_list\")\n",
        "    class_doc_ref = db.collection(\"classes\").document(str(class_date))\n",
        "    class_doc_ref.set(student_id_list_ref.get().to_dict())\n",
        "\n",
        "def student_arrive(student_id):\n",
        "    cur_date = datetime.now(pytz.timezone('Asia/Taipei')).strftime(\"%m%d\")\n",
        "    cur_time = datetime.now(pytz.timezone('Asia/Taipei')).strftime(\"%H:%M:%S\")\n",
        "    class_ref = db.collection(\"classes\").document(str(cur_date))\n",
        "    snapshot = class_ref.get()\n",
        "    snapshot_dict = snapshot.to_dict()\n",
        "    class_update_data = {\n",
        "        student_id: cur_time\n",
        "    }\n",
        "    if class_ref.get().exists == False:\n",
        "        create_class(cur_date)\n",
        "    elif type(snapshot_dict[student_id]) == str:\n",
        "        print(f\"{student_id} had arrived at {snapshot_dict[student_id]}\")\n",
        "        return\n",
        "    class_ref.update(class_update_data)\n",
        "    ##########\n",
        "    student_document_ref = db.collection(\"students\").document(\"student_id_list\").collection(\"ML\").document(student_id)\n",
        "    student_update_data = {\n",
        "        cur_date : cur_time\n",
        "    }\n",
        "    if student_document_ref.get().exists == False:\n",
        "        student_document_ref.set(student_update_data)\n",
        "    else:\n",
        "        student_document_ref.update(student_update_data)\n",
        "\n",
        "    print(f\"update successed. {student_id} arrives at {cur_date} {cur_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpwWhxCaO0vb"
      },
      "source": [
        "# INPUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DLC1hljVsO_"
      },
      "source": [
        "## WebCam setting\n",
        "[REFERENCE](https://colab.research.google.com/github/OmniXRI/Colab_Webcam_OpenCV/blob/main/Colab_Webcam_OpenCV.ipynb#scrollTo=nLxYK8ccMMI-)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q9Q_e6JCX1n-"
      },
      "outputs": [],
      "source": [
        "# https://colab.research.google.com/github/OmniXRI/Colab_Webcam_OpenCV/blob/main/Colab_Webcam_OpenCV.ipynb#scrollTo=nLxYK8ccMMI-\n",
        "from IPython.display import display, Javascript, Image\n",
        "# from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import subprocess\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MToawQgGYqIQ"
      },
      "outputs": [],
      "source": [
        "def js_to_image(js_reply):\n",
        "  # 解碼成 base64 格式影像\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # 轉換影像變成 Numpy 格式\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # 解碼 Numpy格式到 OpenCV BGR 影像格式\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "def image_to_js(cv2_img):\n",
        "  _, buffer = cv2.imencode('.jpg', cv2_img)\n",
        "  image_str = b64encode(buffer).decode('utf-8')\n",
        "\n",
        "  # show img\n",
        "  display(Javascript(\"\"\"\n",
        "      var img = document.createElement('img');\n",
        "      img.src = 'data:image/jpeg;base64,{}';\n",
        "      document.body.appendChild(img);\n",
        "  \"\"\".format(image_str)))\n",
        "\n",
        "def img_process(img):\n",
        "\n",
        "    bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    return bgr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def video_frame(label, bbox):\n",
        "  # data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  result = subprocess.run(['node', 'c:/Users/User/Desktop/study/_ML/AdaFace/video_stream.js', label, bbox], capture_output=True, text=True)\n",
        "  print(\"stdout:\", result.stdout)\n",
        "  print(\"stderr:\", result.stderr)\n",
        "  result.check_returncode()\n",
        "  data = result.stdout.strip()\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaeoLbQ1Vgpu"
      },
      "source": [
        "## Create Video Stream and Start Detection\n",
        "\n",
        "now detect every 5 seconds\n",
        "want to improve to approximately real-time\n",
        "and improve the representation in the scheme\n",
        "\n",
        "-\n",
        "\n",
        "想要可以把人框起來上面寫人名跟score的那種酷效果\n",
        "可能要改yolo_face回傳的參數\n",
        "\n",
        "yolo_face裡面有個detect video還是什麼的但那在colab上用不了Q\n",
        "\n",
        "-\n",
        "\n",
        "1201 update:\n",
        "\n",
        "1. 可以一次測兩個人(歡呼)但我沒有第二台手機沒辦法測第三個人，兩個人ok的話三個人應該也行吧\n",
        "\n",
        "2. firestore database只會紀錄第一次arrive的時間\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from yolo_face.models.experimental import attempt_load\n",
        "from yolo_face.utils.datasets import LoadStreams, LoadImages\n",
        "from yolo_face.utils.general import check_img_size, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box\n",
        "from yolo_face.utils.plots import colors, plot_one_box\n",
        "from yolo_face.utils.torch_utils import select_device, load_classifier, time_synchronized\n",
        "\n",
        "class Opt:\n",
        "    def __init__(self, weights='yolo_face/yolov7-tiny.pt', source='0', img_size=640, conf_thres=0.25,\n",
        "                iou_thres=0.45, device='', view_img=False, save_txt=False, save_txt_tidl=False,\n",
        "                save_bin=False, save_conf=False, save_crop=False, nosave=False, classes=None,\n",
        "                agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp',\n",
        "                exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, kpt_label=5):\n",
        "        self.weights = weights\n",
        "        self.source = source\n",
        "        self.img_size = img_size\n",
        "        self.conf_thres = conf_thres\n",
        "        self.iou_thres = iou_thres\n",
        "        self.device = device\n",
        "        self.view_img = view_img\n",
        "        self.save_txt = save_txt\n",
        "        self.save_txt_tidl = save_txt_tidl\n",
        "        self.save_bin = save_bin\n",
        "        self.save_conf = save_conf\n",
        "        self.save_crop = save_crop\n",
        "        self.nosave = nosave\n",
        "        self.classes = classes\n",
        "        self.agnostic_nms = agnostic_nms\n",
        "        self.augment = augment\n",
        "        self.update = update\n",
        "        self.project = project\n",
        "        self.name = name\n",
        "        self.exist_ok = exist_ok\n",
        "        self.line_thickness = line_thickness\n",
        "        self.hide_labels = hide_labels\n",
        "        self.hide_conf = hide_conf\n",
        "        self.kpt_label = kpt_label\n",
        "\n",
        "opt = Opt()\n",
        "# print(opt)\n",
        "\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     if opt.update:  # update all models (to fix SourceChangeWarning)\n",
        "#         for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt']:\n",
        "#             detect(opt=opt)\n",
        "#             strip_optimizer(opt.weights)\n",
        "#     else:\n",
        "#         detect(opt=opt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "WBlEB-2dY_4_",
        "outputId": "ae9f3482-fb52-4ea2-edc1-49365752e966"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# if cur_time - cam_start_time >= 5:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#     cam_start_time = cur_time\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isyolo:\n\u001b[1;32m---> 31\u001b[0m     aligned_bgr_imgs, coordinate \u001b[38;5;241m=\u001b[39m yoloface\u001b[38;5;241m.\u001b[39mframe_detect(frame)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected faces number: \u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;28mlen\u001b[39m(aligned_bgr_imgs))\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# print(\"type: \", type(aligned_bgr_imgs))\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "# 啟動網路攝影機開始接收串流影像\n",
        "# video_stream()\n",
        "from detect import detect\n",
        "cap = cv2.VideoCapture(cv2.CAP_DSHOW)\n",
        "# 設定串流影像文字標籤\n",
        "label_html = 'Capturing...'\n",
        "# 清空疊合影像內容\n",
        "overlap_img = ''\n",
        "source = './test_images'\n",
        "features = []\n",
        "face=[]\n",
        "imgs=[]\n",
        "test_image = sorted(os.listdir(source))\n",
        "ct=0\n",
        "# 執行取像、處理循環，執行過程中若於影像上按下滑鼠鍵即會中止程式\n",
        "cam_start_time = time.time()\n",
        "while True:\n",
        "    # 從網路攝影機取得串流影像目前影格\n",
        "    ret, frame = cap.read()\n",
        "    # 若無法取得影格則結束循環\n",
        "    if not ret:\n",
        "        print(\"nothing\")\n",
        "        break\n",
        "    \n",
        "    cur_time = time.time()\n",
        "\n",
        "    # if cur_time - cam_start_time >= 5:\n",
        "    #     cam_start_time = cur_time\n",
        "\n",
        "    if isyolo:\n",
        "        aligned_bgr_imgs, coordinate = yoloface.frame_detect(frame)\n",
        "        print(\"Detected faces number: \" , len(aligned_bgr_imgs))\n",
        "        # print(\"type: \", type(aligned_bgr_imgs))\n",
        "        for i,img in enumerate(aligned_bgr_imgs) :\n",
        "            # imgs.append(tt[i])\n",
        "            # cv2.imwrite(f'deteced_{ct}_{i}.jpg',img)\n",
        "            bgr_tensor_input = to_input(img,True).to(device)\n",
        "            feature, _ = model(bgr_tensor_input)\n",
        "            features.append(feature)\n",
        "            face.append(f'deteced_{ct}_{i}.jpg')\n",
        "        ct+=1\n",
        "    else:\n",
        "        # aligned_rgb_img = align.get_aligned_face(js_img, isarray=True)\n",
        "        aligned_rgb_img = align.get_aligned_face(frame, isarray=True)\n",
        "        if(aligned_rgb_img is not None):\n",
        "            bgr_tensor_input = to_input(aligned_rgb_img,False).to(device)\n",
        "            feature, _ = model(bgr_tensor_input)\n",
        "            features.append(feature)\n",
        "\n",
        "    # if nothing detected\n",
        "    if features == []:\n",
        "        continue\n",
        "\n",
        "    database_dir =  'c:/Users/User/Desktop/study/_ML/AdaFace/face_database'\n",
        "    database_pt_dir = os.path.join(database_dir,'database.pt')\n",
        "    if(os.path.exists(database_pt_dir)):\n",
        "        database = torch.load(database_pt_dir).to(device)\n",
        "        with open(os.path.join(database_dir, 'ID_list.txt'), 'r') as file:\n",
        "            ID_list = [line.strip() for line in file.readlines()]\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            similarity_scores = torch.cat(features) @ database.T\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        print(f\"computing time: {execution_time} seconds\")\n",
        "        score = similarity_scores.cpu().detach().numpy()\n",
        "        max_index = np.argmax(score, axis=1)\n",
        "        # print(\"max_index: \",  max_index, \"\\nlen(imgs): \", len(imgs))\n",
        "\n",
        "        for i,id in enumerate(max_index):\n",
        "            print(f'in {face[i]}')\n",
        "            if(score[i][id] >= 0.3):\n",
        "                img_with_text = aligned_bgr_imgs[i]\n",
        "                face_text = f'{ID_list[int(id/3)]}-Score:{score[i][id]:.2f}'\n",
        "                cv2.putText(img_with_text, face_text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)\n",
        "                display(Image(data=cv2.imencode('.png', img_with_text)[1].tobytes(), format='png'))\n",
        "                if cur_time - cam_start_time >= 5:\n",
        "                    cam_start_time = cur_time\n",
        "                    student_arrive(str(ID_list[int(id/3)]))\n",
        "\n",
        "            else:\n",
        "                print(\"no ensured answer\")\n",
        "                img_with_text = aligned_bgr_imgs[i]\n",
        "                face_text = \"no ensured answer\"\n",
        "                cv2.putText(img_with_text, face_text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)\n",
        "                display(Image(data=cv2.imencode('.png', img_with_text)[1].tobytes(), format='png'))\n",
        "            yoloface.plot_one_box(coordinate, frame)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            similarity_scores = torch.cat(features) @ torch.cat(features).T\n",
        "        print(similarity_scores)\n",
        "\n",
        "        # imgs=[]\n",
        "    cv2.imshow('Video Stream', frame)\n",
        "    # detect(opt=opt)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJjAdkeIYrgj"
      },
      "source": [
        "# ORIGINAL CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlMiw_DooZl9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzubGzZEMOEw"
      },
      "outputs": [],
      "source": [
        "# source = 'test_images'\n",
        "# features = []\n",
        "# face=[]\n",
        "# imgs=[]\n",
        "# test_image = sorted(os.listdir(source))\n",
        "# ct=0\n",
        "# for fname in test_image:\n",
        "#     path = os.path.join(source, fname)\n",
        "#     if isyolo:\n",
        "#         aligned_bgr_imgs,tt = yoloface.show_detect(path)\n",
        "#         for i,img in enumerate(aligned_bgr_imgs) :\n",
        "#             imgs.append(tt[i])\n",
        "#             # cv2.imwrite(f'deteced_{ct}_{i}.jpg',img)\n",
        "#             bgr_tensor_input = to_input(img,True).to(device)\n",
        "#             feature, _ = model(bgr_tensor_input)\n",
        "#             features.append(feature)\n",
        "#             face.append(f'deteced_{ct}_{i}.jpg')\n",
        "#         ct+=1\n",
        "#     else:\n",
        "#         aligned_rgb_img = align.get_aligned_face(path, isarray=True)\n",
        "#         if(aligned_rgb_img is not None):\n",
        "#             bgr_tensor_input = to_input(aligned_rgb_img,False).to(device)\n",
        "#             feature, _ = model(bgr_tensor_input)\n",
        "#             features.append(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c64kBjOQLfJ"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import display, Image\n",
        "\n",
        "# database_dir =  '/content/drive/MyDrive/AdaFace/face_database'\n",
        "# database_pt_dir = os.path.join(database_dir,'database.pt')\n",
        "# if(os.path.exists(database_pt_dir)):\n",
        "#     database = torch.load(database_pt_dir).to(device)\n",
        "#     with open(os.path.join(database_dir, 'ID_list.txt'), 'r') as file:\n",
        "#         ID_list = [line.strip() for line in file.readlines()]\n",
        "#     start_time = time.time()\n",
        "#     with torch.no_grad():\n",
        "#         similarity_scores = torch.cat(features) @ database.T\n",
        "#     end_time = time.time()\n",
        "#     execution_time = end_time - start_time\n",
        "#     print(f\"computing time: {execution_time} seconds\")\n",
        "#     score = similarity_scores.cpu().detach().numpy()\n",
        "#     max_index = np.argmax(score, axis=1)\n",
        "\n",
        "#     for i,id in enumerate(max_index):\n",
        "#         print(f'in {face[i]}')\n",
        "#         if(score[i][id] >= 0.3):\n",
        "#             img_with_text = imgs[i].copy()\n",
        "#             face_text = f'{ID_list[int(id/3)]}-Score:{score[i][id]:.2f}'\n",
        "#             cv2.putText(img_with_text, face_text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "#             # plt.figure(figsize=(12,4))\n",
        "#             # img_with_text_rgb = cv2.cvtColor(img_with_text, cv2.COLOR_BGR2RGB)\n",
        "#             display(Image(data=cv2.imencode('.png', img_with_text)[1].tobytes(), format='png'))\n",
        "#             student_arrive(str(ID_list[int(id/3)]))\n",
        "#         else:\n",
        "#             print(\"no ensured answer\")\n",
        "# else:\n",
        "#     with torch.no_grad():\n",
        "#         similarity_scores = torch.cat(features) @ torch.cat(features).T\n",
        "#     print(similarity_scores)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
